import re
import utils
from typing import Any, Dict, List, Tuple, Optional, Union
from configuration import Configuration as Config
from loggers.fuzzy_license_search_logger import fuzzy_license_search_logger as logger
from models.FileData import FileDataManager
from tools import file_content_indexer
from tools.file_content_indexer import FileIndex, PatternIndex, MatchResult, build_file_indexes, \
    build_pattern_indexes_from_dict


def _align_with_gaps(
    file_tokens: List[Dict],
    pattern_tokens: List[str],
    fi_start: int,
    pj_start: int,
    gap_lookahead: int = 5,
) -> Tuple[int, int]:
    """
    Greedy alignment that allows small insertions/deletions on either side.

    Returns:
        (extra_matches, last_match_file_idx)

    - extra_matches: number of matches *after* the starting indices
    - last_match_file_idx: index in file_tokens of the last matched word
                           (or fi_start-1 if none matched).
    """
    n_file = len(file_tokens)
    n_pattern = len(pattern_tokens)

    fi = fi_start
    pj = pj_start
    matches = 0
    last_match_file_idx = fi_start - 1

    while fi < n_file and pj < n_pattern:
        if file_tokens[fi]["norm"] == pattern_tokens[pj]:
            matches += 1
            last_match_file_idx = fi
            fi += 1
            pj += 1
        else:
            # Try to re-sync by looking ahead in file for pattern_tokens[pj]
            found_in_file = None
            for k in range(1, gap_lookahead + 1):
                if fi + k >= n_file:
                    break
                if file_tokens[fi + k]["norm"] == pattern_tokens[pj]:
                    found_in_file = fi + k
                    break

            # Try to re-sync by looking ahead in pattern for file_tokens[fi]
            found_in_pattern = None
            for k in range(1, gap_lookahead + 1):
                if pj + k >= n_pattern:
                    break
                if pattern_tokens[pj + k] == file_tokens[fi]["norm"]:
                    found_in_pattern = pj + k
                    break

            if found_in_file is not None and (
                found_in_pattern is None
                or (found_in_file - fi) <= (found_in_pattern - pj)
            ):
                # Treat as insertion(s) in file: skip a few file tokens
                fi = found_in_file
            elif found_in_pattern is not None:
                # Treat as insertion(s) in pattern: skip a few pattern tokens
                pj = found_in_pattern
            else:
                # Can't re-sync locally, just advance both
                fi += 1
                pj += 1

    return matches, last_match_file_idx


def best_match_indexed(
    f: FileIndex,
    p: PatternIndex,
    anchor_size: int = 3,
    gap_lookahead: int = 5,
) -> Optional[MatchResult]:
    file_tokens = f.tokens
    pattern_tokens = p.tokens
    n_file = len(file_tokens)
    n_pattern = len(pattern_tokens)

    if not file_tokens or n_pattern < anchor_size:
        return None

    # Fast skip: if no shared anchor, no need to align
    common_anchors = f.trigram_positions.keys() & p.anchor_keys
    if not common_anchors:
        return None

    best_result: Optional[MatchResult] = None

    for anchor in common_anchors:
        file_positions = f.trigram_positions[anchor]
        pattern_positions = p.anchor_positions[anchor]

        for i in file_positions:
            for j0 in pattern_positions:
                # We already know the first `anchor_size` words match
                matches = anchor_size
                last_match_file_idx = i + anchor_size - 1

                fi_start = i + anchor_size
                pj_start = j0 + anchor_size

                extra_matches, extra_last_idx = _align_with_gaps(
                    file_tokens,
                    pattern_tokens,
                    fi_start,
                    pj_start,
                    gap_lookahead=gap_lookahead,
                )

                matches += extra_matches
                if extra_matches > 0:
                    last_match_file_idx = extra_last_idx

                start_char = file_tokens[i]["start"]
                end_char = file_tokens[last_match_file_idx]["end"]
                substring = f.text[start_char:end_char]

                match_percent = (matches / n_pattern) * 100.0

                if best_result is None or match_percent > best_result.match_percent:
                    best_result = MatchResult(
                        matched_substring=substring,
                        match_percent=match_percent,
                        start_index=start_char,
                        end_index=end_char,
                    )

    return best_result


_VERSION_RE = re.compile(
    r"""
    \bversion\s+(\d+(?:\.\d+)?)   # "version" <num>
      |\bv\.?\s*(\d+(?:\.\d+)?)     # "v" or "v." <num>
      |\blicense\s+(\d+(?:\.\d+)?)
    """,
    re.IGNORECASE | re.VERBOSE,
)


def _extract_version(text: str) -> Optional[str]:
    """
    Extract the first version number indicated by one of:
      - "version" <number>
      - "v" or "v." <number>
      - "license" <number>

    Returns just the numeric part as a string (e.g. "2", "3.0"), or None if not found.
    """
    m = _VERSION_RE.search(text)
    if not m:
        return None
    for g in m.groups():
        if g is not None:
            return g
    return None


def _extract_versions(text: str) -> Optional[List[str]]:
    """
    Extract all version numbers indicated by one of:
      - "version" <number>
      - "v" or "v." <number>
      - "license" <number>

    Returns a list of numeric parts as strings (e.g. ["2", "3.0"]), or None if none found.
    """
    if not text:
        return []

    versions: List[str] = []

    # Use finditer instead of search to get all matches
    for match in _VERSION_RE.finditer(text):
        # Keep the same "first non-None group" logic per match
        for g in match.groups():
            if g is not None:
                # Only add if it's not already in the list (distinct versions)
                if g not in versions:
                    versions.append(g)
                break  # move to the next match

    return versions


def fuzzy_match_licenses_in_assessment_files(pattern_indexes):
    for f_idx in Config.file_indexes:
        file_model = f_idx.source_obj  # original model instance
        for p_idx in pattern_indexes:
            pattern_path = p_idx.source_path  # the Path key from Dict[Path, str]
            fuzzy_match_result = best_match_indexed(f_idx, p_idx, anchor_size=4)
            if fuzzy_match_result and fuzzy_match_result.match_percent > 50.0:
                license_name = utils.get_file_name_from_path_without_extension(pattern_path)
                fuzzy_match_result.license_name = license_name
                fuzzy_match_result.expected_versions = utils.extract_versions_from_name(license_name)
                found_versions = _extract_versions(fuzzy_match_result.matched_substring)
                fuzzy_match_result.found_versions = utils.normalize_number_strings(found_versions)
                file_model.fuzzy_license_matches.append(fuzzy_match_result)


if __name__ == "__main__":
    Config.file_data_manager = FileDataManager()
    license_headers_normalized = utils.read_and_normalize_licenses([Config.spdx_license_headers_dir, Config.manual_license_headers_dir])
    Config.file_indexes = file_content_indexer.build_file_indexes(Config.file_data_manager.get_all_file_data(), anchor_size=4)
    Config.license_header_indexes = file_content_indexer.build_pattern_indexes_from_dict(license_headers_normalized)
    fuzzy_match_licenses_in_assessment_files(Config.license_header_indexes)
